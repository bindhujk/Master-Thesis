{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b8a44b-2fa5-4498-b4b9-33a79728c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import itertools\n",
    "\n",
    "from config import (DATA_PATH, DATE_COL, SPLIT_COL, TARGET_COL, L, WARMUP,           \n",
    "    SCENARIOS, H_DEFAULT, BETA_GRID_TREE, ETA_GRID_TREE)\n",
    "\n",
    "from helpers import (add_leadtime_target_noleak, build_group_cache, pooled_empirical_from_train,\n",
    "    tune_beta_eta_fast_core, _simulate_rq_multi, aggregate_matrix)\n",
    "\n",
    "RANDOM_SEED: int = 123\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def tune_beta_eta_fast(leaf_ids_val, leaf_ids_train, y_train, betas, K_order, p, h, L,                   \n",
    "    warmup, eta_grid, service_min=0.95, fill_min=0.95):\n",
    "    return tune_beta_eta_fast_core(leaf_ids_val=leaf_ids_val, leaf_ids_train=leaf_ids_train, y_train=y_train,                 \n",
    "        betas=np.asarray(betas, float), K_order=K_order, p=p, h=h, L=L, warmup=warmup,                 \n",
    "        eta_grid=np.asarray(eta_grid, float), service_min=service_min, fill_min=fill_min, val_dem_cat=val_dem_cat,          \n",
    "        val_pos_cat=val_pos_cat, val_starts=val_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb3be5a-2f69-4795-8d7b-28a328acc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wday', 'year', 'sell_price', 'is_event', 'snap', 'day', 'has_event2', 'lag_sales_1', 'lag_sales_7', 'lag_sales_14', 'lag_sales_28', 'lag_sales_56', 'lag_sales_365', 'roll_mean_7', 'roll_std_7', 'roll_mean_28', 'roll_std_28', 'roll_mean_56', 'roll_std_56', 'roll_mean_365', 'roll_std_365', 'lag_price_1', 'roll_price_7', 'price_change_1', 'days_since_start', 'event_name_1_Chanukah End', 'event_name_1_Christmas', 'event_name_1_Cinco De Mayo', 'event_name_1_ColumbusDay', 'event_name_1_Easter', 'event_name_1_Eid al-Fitr', 'event_name_1_EidAlAdha', \"event_name_1_Father's day\", 'event_name_1_Halloween', 'event_name_1_IndependenceDay', 'event_name_1_LaborDay', 'event_name_1_LentStart', 'event_name_1_LentWeek2', 'event_name_1_MartinLutherKingDay', 'event_name_1_MemorialDay', \"event_name_1_Mother's day\", 'event_name_1_NBAFinalsEnd', 'event_name_1_NBAFinalsStart', 'event_name_1_NewYear', 'event_name_1_OrthodoxChristmas', 'event_name_1_OrthodoxEaster', 'event_name_1_Pesach End', 'event_name_1_PresidentsDay', 'event_name_1_Purim End', 'event_name_1_Ramadan starts', 'event_name_1_StPatricksDay', 'event_name_1_SuperBowl', 'event_name_1_Thanksgiving', 'event_name_1_ValentinesDay', 'event_name_1_VeteransDay', 'event_type_1_Cultural', 'event_type_1_National', 'event_type_1_Religious', 'event_type_1_Sporting', 'dayofyear', 'weekday_nb', 'month_nb', 'weekofyear_nb', 'dayofyear_sin', 'dayofyear_cos', 'weekofyear_sin', 'weekofyear_cos', 'month_sin', 'month_cos', 'weekday_sin', 'weekday_cos']\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset\n",
    "df = pd.read_csv(DATA_PATH).copy()\n",
    "\n",
    "# constructing a composite sku key if it is not already present in the dataset\n",
    "if \"sku\" not in df.columns:\n",
    "    df[\"sku\"] = df[\"store_id\"].astype(str) + \"_\" + df[\"item_id\"].astype(str)\n",
    "\n",
    "# parsing the date column \n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "# normalizing the split labels to a consistent lowercase format with no stray spaces\n",
    "df[SPLIT_COL] = df[SPLIT_COL].astype(str).str.lower().str.strip()\n",
    "\n",
    "# ensuring there is a demand column; if not present, mapping from sales and clipping to nonnegative\n",
    "if \"demand\" not in df.columns:\n",
    "    df[\"demand\"] = df[\"sales\"].astype(float).clip(lower=0.0)\n",
    "\n",
    "# sorting the entire table by (sku,date) to make group-by operations deterministic\n",
    "df = df.sort_values([\"sku\", DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "# building the leak-safe lead-time target D_L strictly under same-split windows\n",
    "dl = add_leadtime_target_noleak(df[[\"sku\", DATE_COL, \"demand\", SPLIT_COL]], L=L, split_col=SPLIT_COL)\n",
    "\n",
    "# merging the constructed D_L back to the main frame in a one-to-one fashion\n",
    "df = df.merge(dl[[\"sku\", DATE_COL, TARGET_COL]], on=[\"sku\", DATE_COL], how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "# creating a boolean mask to indicate which rows actually have a finite D_L target\n",
    "has_label = ~df[TARGET_COL].isna()\n",
    "\n",
    "# building split masks\n",
    "is_tr = df[SPLIT_COL].eq(\"train\") & has_label\n",
    "is_va = df[SPLIT_COL].isin([\"val\", \"validation\"]) & has_label\n",
    "is_te = df[SPLIT_COL].eq(\"test\")\n",
    "\n",
    "# defining columns that must never be used as model inputs (keys, labels, raw demand)\n",
    "drop_cols = {\"sales\", \"demand\", TARGET_COL, \"date\", \"sku\", SPLIT_COL, 'dept_roll_mean_28','cumulative_sales'}\n",
    "\n",
    "# selecting only the numeric features\n",
    "feature_cols = [\n",
    "    c for c in df.columns\n",
    "    if (c not in drop_cols) and pd.api.types.is_numeric_dtype(df[c])\n",
    "]\n",
    "print(feature_cols)\n",
    "\n",
    "# extracting TRAIN feature matrix (aligned with the TRAIN mask)\n",
    "X_tr = df.loc[is_tr, feature_cols].reset_index(drop=True)\n",
    "\n",
    "# extracting TRAIN target vector as float64 numpy array\n",
    "y_tr = df.loc[is_tr, TARGET_COL].to_numpy(np.float64)\n",
    "\n",
    "# extracting VAL feature matrix (aligned with the VAL mask)\n",
    "X_va = df.loc[is_va, feature_cols].reset_index(drop=True)\n",
    "\n",
    "# extracting TEST feature matrix (aligned with the TEST mask)\n",
    "X_te = df.loc[is_te, feature_cols].reset_index(drop=True)\n",
    "\n",
    "# collecting the VAL key frame for later merging and output \n",
    "val_keys = df.loc[is_va, [\"sku\", DATE_COL]].reset_index(drop=True)\n",
    "\n",
    "# collecting the TEST key frame for later merging and output \n",
    "test_keys = df.loc[is_te, [\"sku\", DATE_COL]].reset_index(drop=True)\n",
    "\n",
    "# preparing a VAL demand frame (sku,date,demand) to drive the simulator\n",
    "val_sim_df = (df.loc[df[SPLIT_COL].isin([\"val\", \"validation\"]), [\"sku\", DATE_COL, \"demand\"]]\n",
    "      .sort_values([\"sku\", DATE_COL]).reset_index(drop=True))\n",
    "\n",
    "# preparing a TEST demand frame (sku,date,demand) to drive the simulator\n",
    "test_sim_df = (df.loc[df[SPLIT_COL].eq(\"test\"), [\"sku\", DATE_COL, \"demand\"]]\n",
    "      .sort_values([\"sku\", DATE_COL]).reset_index(drop=True))\n",
    "\n",
    "# building the VAL caches that the simulator and tuner expect \n",
    "val_dem_cat, val_pos_cat, val_starts = build_group_cache(val_keys, val_sim_df)\n",
    "\n",
    "# building the TEST caches in the same format to avoid re-joining inside loops\n",
    "test_dem_cat, test_pos_cat, test_starts = build_group_cache(test_keys, test_sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c021c78-0343-4e8f-9b38-f8271b193836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[M5 Cluster+Regressor] scenario K=10.0, p=1.0, h=0.1\n",
      "  tuned (VAL): beta=0.800, eta_Q=1.10 | svc=0.969, fill=0.959, total=974.09\n",
      "\n",
      "[M5 Cluster+Regressor] scenario K=10.0, p=3.0, h=0.1\n",
      "  tuned (VAL): beta=0.800, eta_Q=1.20 | svc=0.970, fill=0.961, total=1232.70\n",
      "\n",
      "[M5 Cluster+Regressor] scenario K=10.0, p=9.0, h=0.1\n",
      "  tuned (VAL): beta=0.940, eta_Q=1.20 | svc=0.988, fill=0.981, total=1737.53\n",
      "\n",
      "[M5 Cluster+Regressor] scenario K=50.0, p=1.0, h=0.1\n",
      "  tuned (VAL): beta=0.600, eta_Q=0.90 | svc=0.961, fill=0.953, total=1521.74\n",
      "\n",
      "[M5 Cluster+Regressor] scenario K=50.0, p=3.0, h=0.1\n",
      "  tuned (VAL): beta=0.600, eta_Q=1.20 | svc=0.970, fill=0.963, total=1763.06\n",
      "\n",
      "[M5 Cluster+Regressor] scenario K=50.0, p=9.0, h=0.1\n",
      "  tuned (VAL): beta=0.880, eta_Q=1.10 | svc=0.990, fill=0.985, total=2212.40\n",
      "\n",
      "[M5 Cluster+Regressor] scenario K=100.0, p=1.0, h=0.1\n",
      "  tuned (VAL): beta=0.600, eta_Q=0.90 | svc=0.971, fill=0.964, total=1995.89\n",
      "\n",
      "[M5 Cluster+Regressor] scenario K=100.0, p=3.0, h=0.1\n",
      "  tuned (VAL): beta=0.600, eta_Q=1.00 | svc=0.975, fill=0.969, total=2200.81\n",
      "\n",
      "[M5 Cluster+Regressor] scenario K=100.0, p=9.0, h=0.1\n",
      "  tuned (VAL): beta=0.800, eta_Q=1.10 | svc=0.989, fill=0.984, total=2615.58\n",
      "Cluster-based + Regressor results summary\n",
      "     K  p    h  L                   model  beta  eta_Q  n_skus   total_cost  \\\n",
      "0   10  1  0.1  7  tree_cluster_regressor  0.80    1.1     500  1092.530345   \n",
      "1   10  3  0.1  7  tree_cluster_regressor  0.80    1.2     500  1328.638393   \n",
      "2   10  9  0.1  7  tree_cluster_regressor  0.94    1.2     500  1821.327492   \n",
      "3   50  1  0.1  7  tree_cluster_regressor  0.60    0.9     500  1736.199351   \n",
      "4   50  3  0.1  7  tree_cluster_regressor  0.60    1.2     500  1979.377934   \n",
      "5   50  9  0.1  7  tree_cluster_regressor  0.88    1.1     500  2429.977037   \n",
      "6  100  1  0.1  7  tree_cluster_regressor  0.60    0.9     500  2317.199077   \n",
      "7  100  3  0.1  7  tree_cluster_regressor  0.60    1.0     500  2489.831083   \n",
      "8  100  9  0.1  7  tree_cluster_regressor  0.80    1.1     500  2891.324664   \n",
      "\n",
      "   beta_fill_rate  ...  demand_served  ordered_qty  receipts_qty  end_on_hand  \\\n",
      "0        0.960705  ...    1018.640834  1091.084673   1087.223972    27.271986   \n",
      "1        0.962795  ...    1023.297866  1090.711970   1087.404023    28.504990   \n",
      "2        0.984170  ...    1058.220659  1096.839756   1092.073966    42.874213   \n",
      "3        0.953328  ...    1008.582309  1093.726224   1089.449278    33.538521   \n",
      "4        0.962907  ...    1023.876618  1095.781653   1089.787348    41.589344   \n",
      "5        0.987097  ...    1061.727286  1097.137333   1096.993298    56.646333   \n",
      "6        0.965158  ...    1028.632245  1093.137884   1089.399904    43.375092   \n",
      "7        0.969812  ...    1033.920328  1093.307255   1088.816331    46.185725   \n",
      "8        0.985675  ...    1059.310685  1098.134438   1091.630375    58.008530   \n",
      "\n",
      "   end_backlog  avg_cost_per_sku_day  avg_cost_per_unit  holding_share  \\\n",
      "0     0.275726              3.987337           1.009704       0.681786   \n",
      "1     0.374532              4.849045           1.227913       0.653430   \n",
      "2     0.081733              6.647181           1.683250       0.720197   \n",
      "3     0.372931              6.336494           1.604576       0.522848   \n",
      "4     0.374891              7.224007           1.829319       0.606382   \n",
      "5     0.002955              8.868529           2.245758       0.658410   \n",
      "6     0.287915              8.456931           2.141529       0.523323   \n",
      "7     0.121760              9.086975           2.301074       0.548941   \n",
      "8     0.003675             10.552280           2.672130       0.620665   \n",
      "\n",
      "   backorder_share  ordering_share  \n",
      "0         0.067049        0.251165  \n",
      "1         0.140947        0.205623  \n",
      "2         0.117199        0.162603  \n",
      "3         0.050402        0.426749  \n",
      "4         0.100483        0.293135  \n",
      "5         0.086022        0.255569  \n",
      "6         0.028753        0.447923  \n",
      "7         0.066817        0.384242  \n",
      "8         0.077582        0.301753  \n",
      "\n",
      "[9 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# initializing a small, regularized regression tree for clustering\n",
    "tree = DecisionTreeRegressor(\n",
    "    max_depth=7,                \n",
    "    min_samples_leaf=800,       \n",
    "    random_state=RANDOM_SEED     \n",
    ")\n",
    "\n",
    "# fitting the tree on TRAIN only to avoid any leakage from VAL/TEST\n",
    "tree.fit(X_tr, y_tr)\n",
    "\n",
    "# computing the TRAIN leaf ids which will be used as the source pools for quantiles/means\n",
    "leaf_tr = tree.apply(X_tr).astype(np.int64)\n",
    "\n",
    "# computing the VAL leaf ids which will be used for tuning (beta,eta)\n",
    "leaf_va = tree.apply(X_va).astype(np.int64)\n",
    "\n",
    "# computing the TEST leaf ids which will be used for inference of (r,Q)\n",
    "leaf_te = tree.apply(X_te).astype(np.int64)\n",
    "\n",
    "all_results = []    \n",
    "all_rQ_rows = []   \n",
    "\n",
    "# iterating over the scenarios \n",
    "for sc in SCENARIOS:\n",
    "    Kc, pc, hc = float(sc[\"K\"]), float(sc[\"p\"]), float(sc[\"h\"])\n",
    "    print(f\"\\n[M5 Cluster+Regressor] scenario K={Kc}, p={pc}, h={hc}\")\n",
    "\n",
    "    # computing pooled VAL quantiles/means by looking up each VAL row’s TRAIN leaf\n",
    "    q_val, mu_val = pooled_empirical_from_train(leaf_va, leaf_tr, y_tr, BETA_GRID_TREE)\n",
    "    # precomputing EOQ (without eta) once for VAL because it does not depend on beta\n",
    "    mu_rate_val = np.maximum(mu_val, 0.0) / float(L)\n",
    "    Q_eoq_val   = np.sqrt((2.0 * Kc * mu_rate_val) / max(hc, 1e-12))\n",
    "\n",
    "    best_feas = None\n",
    "    best_any  = None\n",
    "    best_payload = None\n",
    "\n",
    "    # looping over beta candidates to test different r-levels\n",
    "    for b_idx, beta in enumerate(BETA_GRID_TREE):\n",
    "        # forming the reorder point vector for VAL by taking the beta-quantile from pooled stats\n",
    "        r_val = q_val[:, b_idx]\n",
    "        # expanding r to the per-day level using the prebuilt (sku,date) → position mapping\n",
    "        r_cat = r_val[val_pos_cat]\n",
    "\n",
    "        # looping over eta candidates to scale the economic order quantities into the final Q values\n",
    "        for eta in ETA_GRID_TREE:\n",
    "            # building the full Q vector on VAL by scaling EOQ and respecting the lower bound of 1\n",
    "            Q_val = np.maximum(Q_eoq_val * eta, 1.0)\n",
    "            # expanding Q to the per-day level (same mapping as r)\n",
    "            q_cat = Q_val[val_pos_cat]\n",
    "\n",
    "            # running the simulator on VAL for this (beta, eta) pair\n",
    "            out_mat = _simulate_rq_multi(val_dem_cat, r_cat, q_cat, val_starts, L, Kc, hc, pc, WARMUP)\n",
    "            agg = aggregate_matrix(out_mat)\n",
    "\n",
    "            # extracting the service and fill metrics along with total and holding cost (used for tie-breaks)\n",
    "            svc, fill = agg[\"service_level\"], agg[\"beta_fill_rate\"]\n",
    "            tot, hold = agg[\"total_cost\"], agg[\"holding_cost\"]\n",
    "\n",
    "            # building a lexicographic key for feasible candidates (svc/fill ≥ thresholds)\n",
    "            if (svc >= 0.95) and (fill >= 0.95):\n",
    "                key = (tot, hold, -svc, -fill, float(beta), float(eta))\n",
    "                # updating the best feasible if this candidate is strictly better\n",
    "                if (best_feas is None) or (key < best_feas):\n",
    "                    best_feas = key\n",
    "                    best_payload = dict(beta=float(beta), eta_Q=float(eta),\n",
    "                                        total_mean=float(tot), service_mean=float(svc), fill_mean=float(fill))\n",
    "\n",
    "            # also tracking the best overall candidate by minimizing shortfall first (in case nothing is feasible)\n",
    "            shortfall = max(0.0, 0.95 - svc) + max(0.0, 0.95 - fill)\n",
    "            key_any = (shortfall, tot, -svc, -fill, float(beta), float(eta))\n",
    "            if (best_any is None) or (key_any < best_any):\n",
    "                best_any = key_any\n",
    "                if best_payload is None:  \n",
    "                    best_payload = dict(beta=float(beta), eta_Q=float(eta),\n",
    "                                        total_mean=float(tot), service_mean=float(svc), fill_mean=float(fill))\n",
    "\n",
    "    # deciding which candidate to adopt: feasible if we found one; otherwise the best \"least bad\" option\n",
    "    chosen = best_feas if best_feas is not None else best_any\n",
    "    assert chosen is not None, \"tuning produced no candidate; check inputs\"\n",
    "    beta_star = float(chosen[4])\n",
    "    eta_star  = float(chosen[5])\n",
    "    print(f\"  tuned (VAL): beta={beta_star:.3f}, eta_Q={eta_star:.2f} | \"\n",
    "          f\"svc={best_payload['service_mean']:.3f}, fill={best_payload['fill_mean']:.3f}, \"\n",
    "          f\"total={best_payload['total_mean']:.2f}\")\n",
    "\n",
    "    # obtaining TRAIN-time r at the tuned beta by looking up each TRAIN row’s own leaf (constant within leaf)\n",
    "    q_tr_leaf, mu_tr_leaf = pooled_empirical_from_train(leaf_tr, leaf_tr, y_tr, [beta_star])\n",
    "    # squeezing q_tr to shape (N_train,) because pooled_empirical returns 2D for quantiles\n",
    "    r_tr_per_row = q_tr_leaf[:, 0]\n",
    "    # computing per-row EOQ on TRAIN using the leaf mean, then scaling by eta\n",
    "    mu_rate_tr = np.maximum(mu_tr_leaf, 0.0) / float(L)\n",
    "    Q_tr_per_row = np.maximum(np.sqrt((2.0 * Kc * mu_rate_tr) / max(hc, 1e-12)) * eta_star, 1.0)\n",
    "\n",
    "    # stacking the two labels into a multi-output Y matrix of shape (N_train, 2) in the order [r, Q]\n",
    "    Y_tr_rq = np.column_stack([r_tr_per_row.astype(np.float64), Q_tr_per_row.astype(np.float64)])\n",
    "\n",
    "    # instantiating a compact random forest\n",
    "    rf_rq = RandomForestRegressor(\n",
    "        n_estimators=300,         \n",
    "        max_depth=12,           \n",
    "        min_samples_leaf=50,       \n",
    "        n_jobs=-1,                 \n",
    "        random_state=RANDOM_SEED   \n",
    "    )\n",
    "    # training the regressor on TRAIN features with the constructed (r,Q) labels\n",
    "    rf_rq.fit(X_tr, Y_tr_rq)\n",
    "\n",
    "    # running the regressor on TEST features to obtain a (n_test × 2) matrix of predictions\n",
    "    Y_te_hat = rf_rq.predict(X_te)\n",
    "    # slicing the two columns into separate arrays and enforcing the lower bound on Q\n",
    "    r_te_hat = Y_te_hat[:, 0].astype(np.float64)\n",
    "    Q_te_hat = np.maximum(Y_te_hat[:, 1], 1.0).astype(np.float64)\n",
    "\n",
    "    # mapping the row-level predictions to per-day arrays using the cached (sku,date) → index mapping\n",
    "    r_cat = r_te_hat[test_pos_cat]\n",
    "    q_cat = Q_te_hat[test_pos_cat]\n",
    "\n",
    "    # running the batched TEST simulation for this scenario with the learned (r, Q) paths\n",
    "    out_mat = _simulate_rq_multi(test_dem_cat, r_cat, q_cat, test_starts, L, Kc, hc, pc, WARMUP)\n",
    "    agg = aggregate_matrix(out_mat)\n",
    "\n",
    "    agg.update(dict(model=\"tree_cluster_regressor\", K=int(Kc), p=int(pc), h=float(hc),\n",
    "                    L=int(L), beta=beta_star, eta_Q=eta_star, n_skus=out_mat.shape[0]))\n",
    "    all_results.append(agg)\n",
    "\n",
    "    rQ_te = test_keys.copy()\n",
    "    rQ_te[\"r\"]    = r_te_hat\n",
    "    rQ_te[\"Q\"]    = Q_te_hat\n",
    "    rQ_te[\"K\"]    = int(Kc); rQ_te[\"p\"] = int(pc); rQ_te[\"h\"] = float(hc); rQ_te[\"L\"] = int(L)\n",
    "    rQ_te[\"beta\"] = beta_star; rQ_te[\"eta_Q\"] = eta_star\n",
    "    rQ_te[\"model\"]= \"tree_cluster_regressor\"\n",
    "    all_rQ_rows.append(rQ_te)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "if not results_df.empty:\n",
    "    scenario_cols = [\"K\",\"p\",\"h\",\"L\",\"model\",\"beta\",\"eta_Q\",\"n_skus\"]\n",
    "    other_cols    = [c for c in results_df.columns if c not in scenario_cols]\n",
    "    results_df    = results_df[scenario_cols + other_cols].sort_values([\"K\",\"p\"]).reset_index(drop=True)\n",
    "    results_df.to_csv(\"m5_cluster_regressor_results.csv\", index=False)\n",
    "\n",
    "rQ_out = pd.concat(all_rQ_rows, ignore_index=True) if all_rQ_rows else pd.DataFrame()\n",
    "if not rQ_out.empty:\n",
    "    rQ_out.to_csv(\"m5_cluster_regressor_rQ.csv\", index=False)\n",
    "\n",
    "print(\"Cluster-based + Regressor results summary\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d036d81-05f2-421e-a90b-c668a36be7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HPO] Best tree params: {'max_depth': 10, 'min_samples_leaf': 400} score= (0.0, 1772.1624674862426)\n"
     ]
    }
   ],
   "source": [
    "# defining a compact grid for depth and minimum leaf size to keep HPO fast\n",
    "DEPTHS    = [7, 10, 15]\n",
    "# choosing a few leaf size thresholds to ensure decent pooling in leaves\n",
    "MIN_LEAFS = [400, 800, 1200]\n",
    "\n",
    "best_score   = None\n",
    "best_params  = None\n",
    "best_tree    = None\n",
    "best_leaf_tr = None\n",
    "best_leaf_va = None\n",
    "\n",
    "# scanning the small grid in a deterministic order\n",
    "for md, ml in itertools.product(DEPTHS, MIN_LEAFS):\n",
    "    # creating a candidate regression tree with the current (depth, min leaf)\n",
    "    cand = DecisionTreeRegressor(\n",
    "        max_depth=md,            \n",
    "        min_samples_leaf=ml,          \n",
    "        random_state=RANDOM_SEED      \n",
    "    )\n",
    "    # fitting the candidate on TRAIN only to keep validation honest\n",
    "    cand.fit(X_tr, y_tr)\n",
    "\n",
    "    # computing TRAIN leaves for pooling empirical stats\n",
    "    ltr = cand.apply(X_tr).astype(np.int64)\n",
    "    # computing VAL leaves for the tuning step\n",
    "    lva = cand.apply(X_va).astype(np.int64)\n",
    "\n",
    "    shortfalls, totals = [], []\n",
    "\n",
    "    # iterating across all scenarios \n",
    "    for sc in SCENARIOS:\n",
    "        Kc, pc, hc = float(sc[\"K\"]), float(sc[\"p\"]), float(sc[\"h\"])\n",
    "\n",
    "        # tuning (beta,eta) on VAL with the current candidate tree\n",
    "        best = tune_beta_eta_fast(\n",
    "            leaf_ids_val=lva,               \n",
    "            leaf_ids_train=ltr,           \n",
    "            y_train=y_tr,                 \n",
    "            betas=BETA_GRID_TREE,         \n",
    "            K_order=Kc, p=pc, h=hc,         \n",
    "            L=L, warmup=WARMUP,          \n",
    "            eta_grid=ETA_GRID_TREE,         \n",
    "            service_min=0.95, fill_min=0.95 \n",
    "        )\n",
    "\n",
    "        # extracting the validation metrics that define the score\n",
    "        svc, fill, tot = best[\"service_mean\"], best[\"fill_mean\"], best[\"total_mean\"]\n",
    "\n",
    "        # computing a combined shortfall relative to the service/fill targets\n",
    "        shortfall = max(0.0, 0.95 - svc) + max(0.0, 0.95 - fill)\n",
    "\n",
    "        # recording the shortfall for this scenario\n",
    "        shortfalls.append(shortfall)\n",
    "        # recording the total cost for this scenario\n",
    "        totals.append(tot)\n",
    "\n",
    "    # computing the lexicographic score as (mean shortfall, mean total cost)\n",
    "    score = (float(np.mean(shortfalls)), float(np.mean(totals)))\n",
    "\n",
    "    # updating the incumbent if this candidate scores strictly better\n",
    "    if (best_score is None) or (score < best_score):\n",
    "        best_score   = score                         # storing the new best score\n",
    "        best_params  = dict(max_depth=md, min_samples_leaf=ml)  # storing params\n",
    "        best_tree    = cand                          # keeping the fitted tree\n",
    "        best_leaf_tr = ltr                           # caching TRAIN leaves\n",
    "        best_leaf_va = lva                           # caching VAL leaves\n",
    "\n",
    "print(\"[HPO] Best tree params:\", best_params, \"score=\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a02009c-602d-41bd-ba73-971e35c7cfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=10.0, p=1.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.800, eta_Q=1.00 | svc=0.964, fill=0.953, total=942.13\n",
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=10.0, p=3.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.800, eta_Q=1.20 | svc=0.969, fill=0.961, total=1182.51\n",
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=10.0, p=9.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.940, eta_Q=1.20 | svc=0.988, fill=0.981, total=1667.20\n",
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=50.0, p=1.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.600, eta_Q=1.00 | svc=0.965, fill=0.957, total=1517.53\n",
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=50.0, p=3.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.600, eta_Q=1.10 | svc=0.969, fill=0.961, total=1753.53\n",
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=50.0, p=9.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.880, eta_Q=1.10 | svc=0.990, fill=0.984, total=2099.54\n",
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=100.0, p=1.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.600, eta_Q=0.90 | svc=0.972, fill=0.965, total=2013.57\n",
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=100.0, p=3.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.600, eta_Q=1.00 | svc=0.974, fill=0.968, total=2209.61\n",
      "\n",
      "[M5 Cluster+Regressor (HPO tree)] scenario K=100.0, p=9.0, h=0.1\n",
      "  tuned (VAL, HPO tree): beta=0.880, eta_Q=1.00 | svc=0.992, fill=0.988, total=2563.86\n",
      "Cluster-based + Regressor tuned results summary\n",
      "     K  p    h  L                       model  beta  eta_Q  n_skus  \\\n",
      "0   10  1  0.1  7  tree_cluster_regressor_hpo  0.80    1.0     500   \n",
      "1   10  3  0.1  7  tree_cluster_regressor_hpo  0.80    1.2     500   \n",
      "2   10  9  0.1  7  tree_cluster_regressor_hpo  0.94    1.2     500   \n",
      "3   50  1  0.1  7  tree_cluster_regressor_hpo  0.60    1.0     500   \n",
      "4   50  3  0.1  7  tree_cluster_regressor_hpo  0.60    1.1     500   \n",
      "5   50  9  0.1  7  tree_cluster_regressor_hpo  0.88    1.1     500   \n",
      "6  100  1  0.1  7  tree_cluster_regressor_hpo  0.60    0.9     500   \n",
      "7  100  3  0.1  7  tree_cluster_regressor_hpo  0.60    1.0     500   \n",
      "8  100  9  0.1  7  tree_cluster_regressor_hpo  0.88    1.0     500   \n",
      "\n",
      "    total_cost  beta_fill_rate  ...  demand_served  ordered_qty  receipts_qty  \\\n",
      "0  1051.618911        0.956765  ...    1016.318253  1091.271570   1086.518825   \n",
      "1  1277.449240        0.962985  ...    1025.553467  1091.536221   1087.813515   \n",
      "2  1680.325296        0.984842  ...    1059.304668  1093.601925   1089.710833   \n",
      "3  1729.884351        0.957895  ...    1017.668811  1093.906864   1089.162390   \n",
      "4  1957.284399        0.960420  ...    1022.110980  1095.082665   1089.363024   \n",
      "5  2277.024004        0.987573  ...    1062.556694  1096.897153   1092.473521   \n",
      "6  2336.225520        0.967568  ...    1031.294912  1093.744756   1091.457267   \n",
      "7  2497.889769        0.969740  ...    1035.596096  1095.224167   1089.472125   \n",
      "8  2872.222225        0.990291  ...    1066.017206  1098.667639   1096.290581   \n",
      "\n",
      "   end_on_hand  end_backlog  avg_cost_per_sku_day  avg_cost_per_unit  \\\n",
      "0    23.815482     0.445618              3.838025           0.971894   \n",
      "1    27.521622     0.217438              4.662224           1.180604   \n",
      "2    37.504356     0.156973              6.132574           1.552938   \n",
      "3    35.109540     0.146171              6.313447           1.598740   \n",
      "4    37.352871     0.281827              7.143374           1.808900   \n",
      "5    49.216233     0.133463              8.310307           2.104400   \n",
      "6    43.781510     0.217946              8.526371           2.159113   \n",
      "7    46.923161     0.312596              9.116386           2.308522   \n",
      "8    59.398506     0.099757             10.482563           2.654476   \n",
      "\n",
      "   holding_share  backorder_share  ordering_share  \n",
      "0       0.622222         0.071325        0.306453  \n",
      "1       0.627352         0.142928        0.229720  \n",
      "2       0.697759         0.117337        0.184905  \n",
      "3       0.560170         0.044846        0.394983  \n",
      "4       0.560870         0.104829        0.334300  \n",
      "5       0.627194         0.080282        0.292524  \n",
      "6       0.519963         0.027081        0.452956  \n",
      "7       0.540421         0.066082        0.393498  \n",
      "8       0.596945         0.057234        0.345821  \n",
      "\n",
      "[9 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# computing TEST leaf ids for the selected tree so we can map TEST rows into its partition\n",
    "best_leaf_te = best_tree.apply(X_te).astype(np.int64)\n",
    "\n",
    "all_results_hpo = []     \n",
    "all_rQ_rows_hpo = []    \n",
    "\n",
    "# iterating across the scenarios \n",
    "for sc in SCENARIOS:\n",
    "    # unpacking the scenario parameters (order cost, backorder cost, holding cost)\n",
    "    Kc, pc, hc = float(sc[\"K\"]), float(sc[\"p\"]), float(sc[\"h\"])\n",
    "    print(f\"\\n[M5 Cluster+Regressor (HPO tree)] scenario K={Kc}, p={pc}, h={hc}\")\n",
    "\n",
    "    # computing pooled VAL quantiles/means using best_tree leaves (VAL rows lookup TRAIN leaves of best_tree)\n",
    "    q_va_best, mu_va_best = pooled_empirical_from_train(best_leaf_va, best_leaf_tr, y_tr, BETA_GRID_TREE)\n",
    "    # precomputing EOQ backbone for VAL (depends on mu, K, h, L) and postpone scaling by eta\n",
    "    mu_rate_va = np.maximum(mu_va_best, 0.0) / float(L)\n",
    "    Q_eoq_va   = np.sqrt((2.0 * Kc * mu_rate_va) / max(hc, 1e-12))\n",
    "\n",
    "    best_feas = None\n",
    "    best_any  = None\n",
    "    best_payload = None\n",
    "\n",
    "    # scanning the beta grid first (discrete quantile choices)\n",
    "    for b_idx, beta in enumerate(BETA_GRID_TREE):\n",
    "        # forming the reorder point vector on VAL from pooled quantiles\n",
    "        r_val = q_va_best[:, b_idx]\n",
    "        # mapping the row-level r into the per-day layout via the cached indices\n",
    "        r_cat = r_val[val_pos_cat]\n",
    "\n",
    "        # scanning eta to determine the lot-sizing scale factor\n",
    "        for eta in ETA_GRID_TREE:\n",
    "            # building the final Q on VAL and enforcing Q ≥ 1\n",
    "            Q_val = np.maximum(Q_eoq_va * eta, 1.0)\n",
    "            # mapping to per-day layout for the simulator\n",
    "            q_cat = Q_val[val_pos_cat]\n",
    "\n",
    "            # simulating the full VAL panel for this candidate\n",
    "            out_mat = _simulate_rq_multi(val_dem_cat, r_cat, q_cat, val_starts, L, Kc, hc, pc, WARMUP)\n",
    "            agg = aggregate_matrix(out_mat)\n",
    "\n",
    "            # extracting metrics used for constraints and tie-breaking\n",
    "            svc, fill = agg[\"service_level\"], agg[\"beta_fill_rate\"]\n",
    "            tot, hold = agg[\"total_cost\"], agg[\"holding_cost\"]\n",
    "\n",
    "            # updating the best feasible choice (svc/fill ≥ 0.95)\n",
    "            if (svc >= 0.95) and (fill >= 0.95):\n",
    "                key = (tot, hold, -svc, -fill, float(beta), float(eta))\n",
    "                if (best_feas is None) or (key < best_feas):\n",
    "                    best_feas = key\n",
    "                    best_payload = dict(beta=float(beta), eta_Q=float(eta),\n",
    "                                        total_mean=float(tot), service_mean=float(svc), fill_mean=float(fill))\n",
    "\n",
    "            # updating the best overall fallback by minimizing shortfall first\n",
    "            shortfall = max(0.0, 0.95 - svc) + max(0.0, 0.95 - fill)\n",
    "            key_any = (shortfall, tot, -svc, -fill, float(beta), float(eta))\n",
    "            if (best_any is None) or (key_any < best_any):\n",
    "                best_any = key_any\n",
    "                if best_payload is None:\n",
    "                    best_payload = dict(beta=float(beta), eta_Q=float(eta),\n",
    "                                        total_mean=float(tot), service_mean=float(svc), fill_mean=float(fill))\n",
    "\n",
    "    # extracting the chosen (beta, eta) pair from the trackers\n",
    "    chosen = best_feas if best_feas is not None else best_any\n",
    "    assert chosen is not None, \"tuning produced no candidate; check inputs\"\n",
    "    beta_star = float(chosen[4])\n",
    "    eta_star  = float(chosen[5])\n",
    "    print(f\"  tuned (VAL, HPO tree): beta={beta_star:.3f}, eta_Q={eta_star:.2f} | \"\n",
    "          f\"svc={best_payload['service_mean']:.3f}, fill={best_payload['fill_mean']:.3f}, \"\n",
    "          f\"total={best_payload['total_mean']:.2f}\")\n",
    "\n",
    "    # obtaining r on TRAIN at the tuned beta using best_tree leaf assignments\n",
    "    q_tr_best, mu_tr_best = pooled_empirical_from_train(best_leaf_tr, best_leaf_tr, y_tr, [beta_star])\n",
    "    # converting the quantile matrix (N×1) to a simple vector for convenience\n",
    "    r_tr_per_row = q_tr_best[:, 0]\n",
    "    # computing TRAIN EOQ and scaling by eta to get final Q labels\n",
    "    mu_rate_tr = np.maximum(mu_tr_best, 0.0) / float(L)\n",
    "    Q_tr_per_row = np.maximum(np.sqrt((2.0 * Kc * mu_rate_tr) / max(hc, 1e-12)) * eta_star, 1.0)\n",
    "\n",
    "    # stacking the two targets into a (N_train × 2) label matrix in [r, Q] order\n",
    "    Y_tr_rq = np.column_stack([r_tr_per_row.astype(np.float64), Q_tr_per_row.astype(np.float64)])\n",
    "\n",
    "    # creating a compact random forest for the (r, Q) mapping\n",
    "    rf_rq = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        min_samples_leaf=50,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    # fitting the regressor with TRAIN features and labels\n",
    "    rf_rq.fit(X_tr, Y_tr_rq)\n",
    "\n",
    "    # running predictions for TEST rows to get a (n_test × 2) matrix\n",
    "    Y_te_hat = rf_rq.predict(X_te)\n",
    "    # splitting into separate vectors and enforcing Q ≥ 1\n",
    "    r_te_hat = Y_te_hat[:, 0].astype(np.float64)\n",
    "    Q_te_hat = np.maximum(Y_te_hat[:, 1], 1.0).astype(np.float64)\n",
    "\n",
    "    # mapping row-level predictions to the per-day simulator layout using cached indices\n",
    "    r_cat = r_te_hat[test_pos_cat]\n",
    "    q_cat = Q_te_hat[test_pos_cat]\n",
    "\n",
    "    # simulating the inventory system on TEST with the learned (r, Q) trajectories\n",
    "    out_mat = _simulate_rq_multi(test_dem_cat, r_cat, q_cat, test_starts, L, Kc, hc, pc, WARMUP)\n",
    "    agg = aggregate_matrix(out_mat)\n",
    "\n",
    "    agg.update(dict(model=\"tree_cluster_regressor_hpo\", K=int(Kc), p=int(pc), h=float(hc),\n",
    "                    L=int(L), beta=beta_star, eta_Q=eta_star, n_skus=out_mat.shape[0]))\n",
    "    all_results_hpo.append(agg)\n",
    "\n",
    "    rQ_te = test_keys.copy()\n",
    "    rQ_te[\"r\"]    = r_te_hat\n",
    "    rQ_te[\"Q\"]    = Q_te_hat\n",
    "    rQ_te[\"K\"]    = int(Kc); rQ_te[\"p\"] = int(pc); rQ_te[\"h\"] = float(hc); rQ_te[\"L\"] = int(L)\n",
    "    rQ_te[\"beta\"] = beta_star; rQ_te[\"eta_Q\"] = eta_star\n",
    "    rQ_te[\"model\"]= \"tree_cluster_regressor_hpo\"\n",
    "    all_rQ_rows_hpo.append(rQ_te)\n",
    "\n",
    "results_hpo_df = pd.DataFrame(all_results_hpo)\n",
    "if not results_hpo_df.empty:\n",
    "    scenario_cols = [\"K\",\"p\",\"h\",\"L\",\"model\",\"beta\",\"eta_Q\",\"n_skus\"]\n",
    "    other_cols    = [c for c in results_hpo_df.columns if c not in scenario_cols]\n",
    "    results_hpo_df = results_hpo_df[scenario_cols + other_cols].sort_values([\"K\",\"p\"]).reset_index(drop=True)\n",
    "    results_hpo_df.to_csv(\"m5_cluster_regressor_results_tuned.csv\", index=False)\n",
    "\n",
    "rQ_hpo_out = pd.concat(all_rQ_rows_hpo, ignore_index=True) if all_rQ_rows_hpo else pd.DataFrame()\n",
    "if not rQ_hpo_out.empty:\n",
    "    rQ_hpo_out.to_csv(\"m5_cluster_regressor_rQ_tuned.csv\", index=False)\n",
    "\n",
    "print(\"Cluster-based + Regressor tuned results summary\")\n",
    "print(results_hpo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037d2ca-b5e4-42d7-98b1-e063d950c4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f591dd-7eba-45ce-9fb4-c84b918271ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
